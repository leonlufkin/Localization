{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../torch')\n",
    "from conv_emergence import NeuralNet, compute_entropy\n",
    "\n",
    "def plot_receptive_field(weights, figsize=(10, 5)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    weights = weights[np.argsort(compute_entropy(weights))]\n",
    "    ax.imshow(weights, cmap='gray')#, vmin=-1, vmax=1)\n",
    "    ax.set_xlabel('Input neurons')\n",
    "    ax.set_ylabel('Hidden neurons')\n",
    "    return fig, ax\n",
    "\n",
    "def plot_metrics(metrics, figsize=(10, 5)):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n",
    "    ax1.plot(metrics['losses'])\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(metrics['accs'])\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    # num_epochs = len(metrics['losses'])\n",
    "    # every_epoch = min(max(num_epochs // 100, 1), 500)\n",
    "    # ipr_epochs = np.append(np.arange(0, num_epochs, every_epoch), num_epochs - 1)\n",
    "    ax3.plot(metrics['iprs'])\n",
    "    ax3.set_xlabel('Epoch intervals')\n",
    "    ax3.set_ylabel('IPR')\n",
    "    \n",
    "    return fig, (ax1, ax2, ax3)\n",
    "\n",
    "def load_and_viz(xi1, xi2, gain, L, K, dim, batch_size, num_epochs, loss, lr, activation, second_layer, figsize1=(10, 4), figsize2=(12, 4)):\n",
    "    key = f'__xi1={xi1:05.2f}_xi2={xi2:05.2f}_gain={gain:05.2f}_L={L:03d}_K={K:03d}_dim={dim}_batch_size={batch_size}_num_epochs={num_epochs}_loss={loss}_lr={lr:.3f}_activation={activation}_second_layer={second_layer}'\n",
    "    model = NeuralNet(L, K, activation, second_layer)\n",
    "    model.load_state_dict(torch.load(f'./results/weights_{key}.pt'))\n",
    "    weights = model.ff1.weight.detach().numpy()\n",
    "    \n",
    "    title = f'xi1={xi1:.2f}, xi2={xi2:.2f}, gain={gain:.2f}, L={L}, K={K}, dim={dim}, batch_size={batch_size}, num_epochs={num_epochs}, loss={loss}, lr={lr:.3f}, activation={activation}, second_layer={second_layer}'\n",
    "    fig, ax = plot_receptive_field(weights, figsize=figsize1)\n",
    "    fig.suptitle(title)\n",
    "    fig.savefig(f'./figs/receptive_field_{key}.png', dpi=300)\n",
    "    # plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    metrics = np.load(f'./results/metrics_{key}.npz', allow_pickle=True)\n",
    "    fig, (ax1, ax2, ax3) = plot_metrics(metrics, figsize=figsize2)\n",
    "    fig.suptitle(title)\n",
    "    fig.savefig(f'./figs/metrics_{key}.png', dpi=300)\n",
    "    # plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return weights, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter range  \n",
    "xi1 = (np.sqrt(20), 10.)\n",
    "xi2 = (0.1, np.sqrt(10),)\n",
    "gain = (0.1, 1.1, 3.0,)\n",
    "L = (20, 40, 100,)\n",
    "K = (10, 40, 100,)\n",
    "dim = (1,)\n",
    "batch_size = 1000\n",
    "num_epochs = 1000000\n",
    "lr = 0.1\n",
    "second_layer = ('linear', 'learnable_bias', 0.,)\n",
    "path = ('/ceph/scratch/leonl/results',)  \n",
    "activation = 'tanh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonlufkin/Documents/GitHub/Localization/leonl/../torch/conv_emergence.py:51: RuntimeWarning: divide by zero encountered in divide\n",
      "  prob = count / np.sum(count)\n",
      "/Users/leonlufkin/Documents/GitHub/Localization/leonl/../torch/conv_emergence.py:51: RuntimeWarning: invalid value encountered in divide\n",
      "  prob = count / np.sum(count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=0.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=0.1, L=100, K=40, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=0.1, L=100, K=100, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=1.1, L=100, K=10, second_layer=learnable_bias\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=1.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=1.1, L=100, K=100, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=3.0, L=100, K=10, second_layer=linear\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=3.0, L=100, K=10, second_layer=learnable_bias\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=3.0, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=3.1622776601683795, gain=3.0, L=100, K=40, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=0.1, gain=0.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=0.1, gain=1.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=10.0, xi2=0.1, gain=3.0, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=0.1, L=100, K=10, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=0.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=0.1, L=100, K=40, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=0.1, L=100, K=40, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=0.1, L=100, K=100, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=0.1, L=100, K=100, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=10, second_layer=linear\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=10, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=40, second_layer=linear\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=40, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=40, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=100, second_layer=linear\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=100, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=1.1, L=100, K=100, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=10, second_layer=linear\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=10, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=40, second_layer=linear\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=40, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=40, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=100, second_layer=linear\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=100, second_layer=learnable_bias\n",
      "Failed: xi1=4.47213595499958, xi2=3.1622776601683795, gain=3.0, L=100, K=100, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=0.1, gain=1.1, L=100, K=10, second_layer=0.0\n",
      "Failed: xi1=4.47213595499958, xi2=0.1, gain=3.0, L=100, K=10, second_layer=0.0\n"
     ]
    }
   ],
   "source": [
    "xi1 = (10., np.sqrt(20),)#[1]\n",
    "xi2 = (np.sqrt(10), 0.1,)#[1]\n",
    "gain = (0.1, 1.1, 3.0,)#[0]\n",
    "L = (20, 40, 100,)#[0]\n",
    "K = (10, 40, 100,)#[2]\n",
    "second_layer = ('linear', 'learnable_bias', 0.,)#[2]\n",
    "from itertools import product\n",
    "for xi1_, xi2_, gain_, L_, K_, second_layer_ in product(xi1, xi2, gain, L, K, second_layer):\n",
    "    try:\n",
    "        weights, metrics = load_and_viz(xi1_, xi2_, gain_, L_, K_, 1, batch_size, num_epochs, 'mse', lr, activation, second_layer_)\n",
    "    except:\n",
    "        print(f'Failed: xi1={xi1_}, xi2={xi2_}, gain={gain_}, L={L_}, K={K_}, second_layer={second_layer_}')\n",
    "# weights, metrics = load_and_viz(xi1, xi2, gain, L, K, 1, batch_size, num_epochs, 'mse', lr, activation, second_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
