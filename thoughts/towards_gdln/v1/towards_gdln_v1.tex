% GO TO: /Users/leonlufkin/Library/texmf/tex/latex
%        to add custom packages to the path
\documentclass{article}
\usepackage{report}

\title{Receptive Field Localization}
\author{Leon Lufkin}
\date{\today}

\makeatletter
\let\Title\@title
\let\Author\@author
\let\Date\@date

\begin{document}

%%%%%%%%%%%%%%%%%
%% The Problem %%
%%%%%%%%%%%%%%%%%
\section{The Problem}
We consider a feedforward neural network with a single hidden layer and activation function $\sigma$.
It receives an input $x \in \R^n$ and produces a scalar output $\hat{y} \in \R$.
The hidden layer has $K$ units.
The weights for the first and second layer are $W_1 \in \R^{K \times n}$ and $W_2 \in \R^{1 \times K}$, respectively, and the corresponding biases are $b_1 \in \R^K$ and $b_2 \in \R$.
\begin{align}
  \hat{y} &= W_2 \sigma( W_1 x + b_1 ) + b_2. \label{eq:model}
\end{align}

Our data $x$ are sampled from a mixture of two translation-invariant distributions in some family $\{ p_\xi \}_{\xi}$ parameterized by a correlation length-scale $\xi$.
That is, we sample $x \sim p_{\xi_1}$ with probability $\frac{1}{2}$ and $x \sim p_{\xi_2}$ otherwise.
If $x$ is sampled from $p_{\xi_1}$, then $y(x) = 1$; otherwise, $y(x) = 0$. 
We can train using either mean-squared error or cross-entropy loss, though we primarily consider the former. 

Alessandro's paper primarily considers the case where $W_2 = \frac{1}{K} \mathbf{1}^\top$ (take the mean of the hidden activations) is fixed and $\sigma(h) = \text{erf}(\frac{h}{\sqrt{2}})$.
I have also tried $\sigma = \text{sigmoid}, \text{ReLU}$.
For the former, the results are qualitatively identical, while for the latter we get localization if $\xi_1 > \xi_2$ and short-range oscillations otherwise.
For $\sigma = \text{ReLU}$, one can further remove the bias terms $b_1$ and $b_2$ (though not for sigmoid).
% TODO: explain why

We consider two types of datasets: the nonlinear Gaussian process (NLGP) and the single pulse (SP).
We explain them in more detail later.
They differ primarily in that the former has continuous support on $\R^n$, while the latter has discrete support on a subset of $\{ 0, 1 \}^n$.
The former also has a gain parameter that controls the degree of localization, while the latter does not. 

Motivated by localization with the ReLU model, we tried to see how well a gated deep linear network (GDLN) could do.
Next, we'll provide some experimental support for why this might work, and then do some analysis. 
We'll conclude with some questions, concerns, and ideas.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Have You Tried Making It Linear? %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Have You Tried Making It Linear?}

Gating lets us decompose the ReLU post-activation in terms of the pre-activation's sign and magnitude.
\begin{align}
  \text{ReLU}(\langle w_1(t), x \rangle)
  &= g(t, x) \langle w_1(t), x \rangle \qquad \text{where} \qquad g(t, x) = \mathbbm{1}( \langle w_1(t), x \rangle \geq 0 ). \label{eq:relu_gating}
\end{align}
We generally assume that $g$ does not vary during learning, even though $w_1$ may.
Later on, we'll try to analyze what happens when this does not hold.

To assess the validity of this assumption, we need to see how much $g(x)$, as defined above, changes during learning.
Additionally, post-hoc, we can usually pick a somewhat sensible gating structure that mimics a specific run's behavior.
But we'd like to be able to determine this gating upfront.
We explore all this in the following subsections.

\subsection{Sign Flipping}

Note that $g$ is invariant to the scale of $w_1$.
We've observed in previous experiments that $w_1$ appears to grow uniformly in size during much of its training.
(There is, importantly, a phase where it goes from Gaussian to non-Gaussian, but the localization seems to be more likely to occur around its mode.)
This suggests that $g(x)$ may be relatively constant during learning.
If this is so, then it would be reasonable to try using a standard GDLN to model the ReLU network.

We will model the ReLU network as in \cref{eq:relu_gating}, focusing on how $g$ varies with time for each hidden neuron.
We will look at the metrics
\begin{align}
  p(t) &= \PR_x( g(t, x) = g(t + \delta t, x) ) \qquad \text{for all $t$} \\
  p_{\text{unif}} &= \PR_x( \{ g(t, x) = g(t', x) \ \forall t,t' \} )
\end{align}




\subsection{Predicting Loca(liza)tion}

\subsection{Evolving Gates?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Let's Consider a Single Layer with Linear Activation... %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Let's Consider a Single Layer with Linear Activation...}

%% Model %%
\subsection{Model}
Our GDLN model is defined as follows:
\begin{align}
  \hat{y}(x) &= \frac{1}{K} \left( \sum_{k \in [K]} g_k(x) w_k^\top \right) x, \label{eq:gdln_model}
\end{align}
where $g_k$ are (node) gates, and $w_k \in \R^n$ are the rows of the first-layer weight matrix $W_1 \in \R^{K \times n}$.
That is,
\begin{align}
  W_1 &= \begin{pmatrix} w_1^\top \\ \vdots \\ w_K^\top \end{pmatrix}
\end{align}

%% Gradient Flow %%
\subsection{Gradient Flow}
Recalling the GDLN paper, the gradient flow for $w_1$ is given by
\begin{align}
  \tau \frac{d}{dt} w_1^\top &= \frac{1}{K} \left[ \Sigma^{yx}(p_1) - \sum_{k \in [K]} w_k^\top \Sigma^{xx}(p_1,p_k) \right], \label{eq:grad_flow}
\end{align}
where
\begin{align}
  \Sigma^{yx}(p_i) &= \left\langle g_i y x^\top \right\rangle_{g,x,y} \\
  \Sigma^{xx}(p_i,p_j) &= \left\langle g_i g_j x x^\top \right\rangle_{g,x}.
\end{align}
% Because we typically take $g$ to be a deterministic function of $x$, I will often suppress the dependence on $g$ in the expectation.

%%%%%%%%%%%%%%%%%%
%% General Case %%
\subsection{General Case}
Let us relabel %$w_i = W_i^\top$, 
$b_i = \Sigma^{yx}(p_i)^\top$ and $A_{ij} = \Sigma^{xx}(p_i,p_j)$.
Note that $A_{ij}$ is symmetric and $A_{ij} = A_{ji}$.
Then, we can write the gradient flow for all weights as
\begin{align}
  K \tau \frac{d}{dt} \underbrace{\begin{bmatrix} w_1 \\ \vdots \\ w_K \end{bmatrix}}_{w \in \R^{Kn}}
  &= \underbrace{\begin{bmatrix} b_1 \\ \vdots \\ b_k \end{bmatrix}}_{b \in \R^{Kn}} - \underbrace{\begin{bmatrix} A_{11} & \cdots & A_{1K} \\ \vdots & \ddots & \vdots \\ A_{K1} & \cdots & A_{KK} \end{bmatrix}}_{A \in \R^{Kn \times Kn}} \begin{bmatrix} w_1 \\ \vdots \\ w_K \end{bmatrix}. \label{eq:grad_flow_matrix}
\end{align}
Observe that $w$ is the vectorized form of our $K \times n$ first-layer weight matrix.
Note also that $A$ is a symmetric real matrix, so we can diagonalize it as $A = P \Lambda P^\top$, where the columns of $P$ are the eigenvectors of $A$ and the diagonal entries of $\Lambda$ are the corresponding (nonnegative) eigenvalues.
(It is symmetric because $A$ is block symmetric with blocks $A_{ij}$, and the blocks are also symmetric.)
To see this more clearly, let us write
\begin{align}
  \tilde{g}(x) &= \begin{bmatrix} g_1(x) \\ \vdots \\ g_K(x) \end{bmatrix}. % \in \{ 0,1 \}^K.
\end{align}
Then,
\begin{align}
  A &= \left\langle (\tilde{g} \otimes x) (\tilde{g} \otimes x)^\top \right\rangle_{g,x}, \label{eq:A_kron}
\end{align}
which is clearly a symmetric matrix.
(Interjection: Whatever distribution we have over $g$ should satisfy that $\tilde{g} \sim \Pi \tilde{g}$, where $\Pi$ is some permutation matrix on $K$ elements.
That is, the distribution should be invariant to the ordering of the gates, since this is what we want empirically.
% So, for a finite dataset with $|X|$ elements and binary gates, there are $\frac{2^{K |X|}}{K !}$ unique gating architectures, up to permutation.
% FIXME: this can't be true...
)

We can reparameterize in terms of $u = P^\top w$ and $c = P^\top b$.
\begin{align}
  K \tau \frac{d}{dt} u &= - \Lambda u + c
  \implies
  u(t) = \Lambda^{-1} e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + \Lambda^{-1} c,
\end{align}
where $C$ is a constant diagonal matrix that defines the initial condition.
So,
\begin{align}
  w(t) 
  &= P \Lambda^{-1} e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + P \Lambda^{-1} c \\
  &= A^{-1} P e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + A^{-1} b. \label{eq:grad_flow_solution}
\end{align}

%% Winning Gating Structure %%
\subsection{Winning Gating Structure}
Can we read off the winning gating structure from the gradient flow?
For simplicity, let us assume we are sampling uniformly from a finite set of $G$ gates, $\{ g \}$.
Then, we can write \cref{eq:grad_flow_matrix} as
\begin{align}
  % K \tau \frac{d}{dt} w_1 &= \frac{1}{G} \sum_{g \in \{ g \}} \left[ \left\langle g_1 y x^\top \right\rangle_{x,y \mid g} - \sum_{k \in [K]} \left\langle g_1 g_k y x^\top \right\rangle_{x,y \mid g} w_k \right].
  K \tau \frac{d}{dt} w &= \frac{1}{G} \sum_{g \in \{ g \}} \left[ b_{x,y \mid g} - A_{x,y \mid g} w \right],
\end{align}
where the subscript on $b$ and $A$ indicates the conditioning on a specific gating structure $g$.
Intuitively, a gating structure that minimizes the norm of $A$ will shrink the slowest.
This is somewhat equivalent to minimizing the eigenvalues of $A$, since they are all nonnegative.
(What happens if an eigenvalue is zero?)

Let us consider a single block in $A$:
\begin{align}
  A_{ij} &= \langle g_i g_j x x^\top \rangle_{g,x}
  = \PR( g_i = 1, g_j = 1 ) \langle x x^\top \rangle_{x \mid g_i = g_j = 1}.
\end{align}

Let us quickly ask: Does what we observe empirically match this intuition?
We see that receptive fields come in pairs and tile the space.


% The hope is that one of these terms dominates the others, so that we can read off the winning gating structure.
% In terms of \cref{eq:grad_flow_solution},
% \begin{align}
%   w(t) &= 
% \end{align}
TODO: empirically look at dominating eigenvalues for finite case!

%% Early Dynamics %%
\subsubsection{Early Dynamics}
For small $t$, but sufficiently large to see separation among different eigenvalues, can we predict the leading structure?

%% Limiting Behavior %%
\subsubsection{Limiting Behavior}
If none of the eigenvalues are zero, then $w(\infty) = A^{-1} b$.
If we write
\begin{align}
  \tilde{x} &= \begin{bmatrix} g_1(x) x \\ \vdots \\ g_K(x) x \end{bmatrix} \in \R^{Kn},
\end{align}
then $A = \langle \tilde{x} \tilde{x}^\top \rangle_{x,y,g}$ and $b = \langle \tilde{x} y \rangle_{x,y,g}$.
Then, it is clear that this is the population solution to the OLS problem of regressing $y$ on $\tilde{x}$, averaging across the distributions of the data \textit{and} the gating architectures.

In this context, one might ask, which gating structure minimizes the MSE loss?
The loss is
\begin{align}
  \LL_{OLS} &= \left\langle \left( \tilde{x}'^\top (\langle \tilde{x} \tilde{x}^\top \rangle_{x,y,g})^{-1} \langle \tilde{x} y \rangle_{x,y,g} - y' \right)^2 \right\rangle_{x',y',g'} \\
  &= \left\langle ( \tilde{x}'^\top (\langle \tilde{x} \tilde{x}^\top \rangle_{x,y,g})^{-1} \langle \tilde{x} y \rangle_{x,y,g} )^2 - 2 ( y' \tilde{x}'^\top (\langle \tilde{x} \tilde{x}^\top \rangle_{x,y,g})^{-1} \langle \tilde{x} y \rangle_{x,y,g} ) + (y')^2 \right\rangle_{x',y',g'} \\
  &= \frac{1}{2} - \langle y \tilde{x}^\top \rangle_{x,y,g} (\langle \tilde{x} \tilde{x}^\top \rangle_{x,y,g})^{-1} \langle \tilde{x} y \rangle_{x,y,g} \\
  &= \frac{1}{2} - \frac{1}{2} \langle \tilde{x} \rangle_{x, g \mid y=1}^\top (\langle \tilde{x} \tilde{x}^\top \rangle_{x, g \mid y=1} + \langle \tilde{x} \tilde{x}^\top \rangle_{x, g \mid y=0})^{-1} \langle \tilde{x} \rangle_{x, g \mid y=1}. \label{eq:ols_loss}
\end{align}
In the final step, we assumed (WLOG) that the negative class is $y=0$ and the positive class is $y=1$.
(Throughout, we also assume that the classes are balanced.)
The question is: For fixed $p_{\xi_1}$ and $p_{\xi_2}$, how do we choose the gates $g_k$ to minimize \cref{eq:ols_loss}?

It may be useful to write this in terms of Kronecker products.
Let
\begin{align}
  \tilde{g}(x) &= \begin{bmatrix} g_1(x) \\ \vdots \\ g_K(x) \end{bmatrix} \in \{ 0,1 \}^K.
\end{align}
Then, minimizing \cref{eq:ols_loss} is equivalent to maximizing
\begin{align}
  \LL^*(\tilde{g}) &= \langle \tilde{g} \otimes x \rangle_{x, g \mid y=1}^\top (\langle ( \tilde{g} \otimes x ) ( \tilde{g} \otimes x )^\top \rangle_{x, g \mid y=1} + \langle ( \tilde{g} \otimes x ) ( \tilde{g} \otimes x )^\top \rangle_{x, g \mid y=0})^{-1} \langle \tilde{g} \otimes x \rangle_{x, g \mid y=1} \label{eq:ols_kron}
\end{align}
% So, we want to maximize \cref{eq:ols_kron} 
over $\tilde{g} : \supp(p_{\xi_1}) \cup \supp(p_{\xi_2}) \to \{0,1\}^K$.
\emph{I will have to think more about this.}

\emph{After a bit more thinking...}
% Let's momentarily remove the precision matrix.
% Then, we are left with $\norm{ \langle \tilde{x} \rangle_{x \mid y=1} }_2^2$.
% The best gating structure would make all the gates identical, because of the symmetry of $\tilde{x}$.
% Let us now reconsider the precision matrix.
% As we get close to having all identical gates, the precision matrix's determinant blows up to infinity??
I think that the best precision matrix would be maximally diagonal (no clue if this is actually true! but maybe it holds empirically?).
For Gaussian data (at least), this mean that the blocks are independent conditioned on all the other blocks.
Gates that tile the space without overlap would achieve this (I think?).
But tbh I haven't got the slightest clue!!




%% Example 1: Exclusive gates %%
\subsection{Exclusive Gates}
Let us assume that the gates are exclusive, that is, only one gate is active at a time.
Then, $\Sigma^{xx}(p,q) = 0$ for $p \neq q$.

% Note that $\Sigma^{xx}(p_1,p_2) = 0$ by the construction of our gates, since they are never both nonzero.
Then $A$ becomes block diagonal.
We can write the gradient flow for $w_1$ as
% \begin{align}
%   \tau \frac{d}{dt} W_1 &= \frac{1}{K} \left[ \Sigma^{yx}(p_1) - W_1 \Sigma^{xx}(p_1,p_1) \right]. \label{eq:grad_flow_exclusive}
% \end{align}
% So, we will only need to compute $\Sigma^{yx}(p_1)$ and $\Sigma^{xx}(p_1,p_1)$.

% We also note that $W_1$ is a $1 \times n$ matrix.
% Again, we relabel for simplicity. % in terms of $w_1 = W_1^\top$, which we view as an $n$-dimensional vector.
% % Let us also write $b_1 = \Sigma^{yx}(p_1)^\top$ and $A = \Sigma^{xx}(p_1,p_1)$.
% So,
\begin{align}
  K \tau \frac{d}{dt} w_1 &= - A_{11} w_1 + b_1.
\end{align}

Note that $A_{11} = \Sigma^{xx}(p_1, p_1)$ is always symmetric (and real).
So, we can diagonalize it as $A_{11} = P \Lambda P^\top$, where the columns of $P$ are $v_1, \ldots, v_n$ and the diagonal entries of $\Lambda$ are $\lambda_1, \ldots, \lambda_n$.
Let us introduce $u_1 = P^\top w_1$ and $c_1 = P^\top b_1$.
Then,
\begin{align}
  K \tau \frac{d}{dt} u_1 &= - \Lambda u_1 + c_1. \label{eq:grad_flow_exclusive_reparam}
\end{align}
This ODE is solved by
\begin{align}
  u_1(t) &= \Lambda^{-1} e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + \Lambda^{-1} c_1,
\end{align}
where $C$ is a constant diagonal matrix that defines the initial condition.
Then,
\begin{align}
  w_1(t) &= P \left( \Lambda^{-1} e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + \Lambda^{-1} c \right) \\
  &= P \Lambda^{-1} e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + P \Lambda^{-1} c_1 \\
  &= A_{11}^{-1} \left( P e^{ -\frac{t}{K \tau} \Lambda + C } \mathbf{1} + b_1 \right).
\end{align}
% We can write $C$ in terms of $w(0)$, the initial condition, but it's messy:
% \begin{align}
%   % w(0) &= A^{-1} (P e^{ C } \mathbf{1} + b) \\
%   % P^\top( A w(0) - b ) &= e^{C} \mathbf{1} \\
%   % C_{ii} &= \log( (P^\top( A w(0) - b ))_i ),
%   C_{ii} &= \log( v_i^\top ( A_{11} w_1(0) - b_1 ) ),
% \end{align}
% so I'll just stick with $C$.

% where we absorb $P$ into $C$ since it is some arbitrary constant vector.
Recalling $A_{11}$ and $b_1$,
\begin{align}
  w_1(t) &= ( \Sigma^{xx}(p_1,p_1) )^{-1} \left( P e^{ -\frac{t}{\tau} \Lambda + C } \mathbf{1} + \Sigma^{yx}(p_1)^\top \right).
\end{align}
So,
\begin{align}
  % W_1(\infty) &= w^\top(\infty) = (\Sigma^{yx}(p_1)) ( \Sigma^{xx}(p_1,p_1) )^{-1}.
  w_1(\infty) &= ( \Sigma^{xx}(p_1,p_1) )^{-1} \left( P e^{ -\frac{t}{\tau} \Lambda + C } \mathbf{1} + \Sigma^{yx}(p_1)^\top \right).
\end{align}
As with above, this is the population solution to OLS, $(X^\top X)^{-1} X^\top y = ( \langle x x^\top \rangle )^{-1} ( \langle x y \rangle )$.

So, each weight matrix converges to the OLS solution on the subset of the data determined by its gate.

% In the limit,
% \begin{align}
%   \hat{y}(x) &= \frac{1}{K} \left( \sum_{k\in[K]} g_k(x) ( \Sigma^{xx}(p_k,p_k) )^{-1} \Sigma^{yx}(p_k)^\top \right) x \\
%   &= \frac{1}{K} \left( \sum_{k\in[K]} ( \Sigma^{xx}(p_k,p_k) )^{-1} \Sigma^{yx}(p_k)^\top \right) x \\
% \end{align}

\subsection{Redundant Gates}
What if $g_1 = g_2$?
Then, $b_1 = b_2$ and $A_{11} = A_{12} = A_{22}$.
So,
% \begin{align}
%   \tau \frac{d}{dt} W_1 
%   &= \frac{1}{K} \left[ \Sigma^{yx}(p_1) - W_1 \Sigma^{xx}(p_1,p_1) - W_2 \Sigma^{xx}(p_1,p_2) \right], \\
%   \tau \frac{d}{dt} W_2 &= \frac{1}{K} \left[ \Sigma^{yx}(p_2) - W_1 \Sigma^{xx}(p_2,p_1) - W_2 \Sigma^{xx}(p_2,p_2) \right].
% \end{align}
\begin{align}
  K \tau \frac{d}{dt} w_1 &= - A_{11} (w_1 + w_2) + b_1 , \\
  K \tau \frac{d}{dt} w_2 &= - A_{11} (w_1 + w_2) + b_1 .
\end{align}
Clearly, then, $w_1 - w_2$ is a constant vector.
Moreover, $\frac{1}{2} (w_1 + w_2)$ evolves according to
\begin{align}
  \frac{K \tau}{2} \frac{d}{dt} (w_1 + w_2) &= - A_{11} (w_1 + w_2) + b_1.
\end{align}
Writing $2 \Delta = w_1 - w_2$ and $w_1 + w_2 = 2 (w_1 - \Delta)$, we have
\begin{align}
  K \tau \frac{d}{dt} w_1
  &= K \tau \frac{d}{dt} (w_1 - \Delta)
  = - 2 A_{11} (w_1 - \Delta) + b_1.
\end{align}
We can plug this into our solution from the previous section to get
\begin{align}
  w_1(t) &= \frac{1}{2} A_{11}^{-1} \left( P e^{-\frac{t}{K \tau} \Lambda + C} \mathbf{1} + b_1 \right).
\end{align}
So,
\begin{align}
  w_2(t) &= \frac{1}{2} A_{11}^{-1} \left( P e^{-\frac{t}{K \tau} \Lambda + C} \mathbf{1} + b_1 \right) - (w_1(0) - w_2(0)).
\end{align}


% Note $\Sigma^{xx}(p_1,p_2) = \Sigma^{xx}(p_2,p_1)$.
% Define $A_{ij} = \Sigma^{xx}(p_i,p_j)$ and $b_i = \Sigma^{yx}(p_i)^\top$.
% Let $w_i = W_i^\top$.
% Then,
% \begin{align}
%   K \tau \frac{d}{dt} w_1 &= b_1 - A_{11} w_1 - A_{12} w_2, \\
%   K \tau \frac{d}{dt} w_2 &= b_2 - A_{12} w_1 - A_{22} w_2.
% \end{align}

% So,
% \begin{align}
%   &\frac{K \tau}{2} \frac{d}{dt} \langle w_1 - w_2, w_1 - w_2 \rangle \\
%   &= K \tau \langle w_1 - w_2, \frac{d}{dt} w_1 - \frac{d}{dt} w_2 \rangle \\
%   &= \langle w_1 - w_2, (b_1 - A_{11} w_1 - A_{12} w_2) - (b_2 - A_{12} w_1 - A_{22} w_2) \rangle \\
%   &= \langle w_1 - w_2, (b_1 - b_2) + (A_{12} - A_{11}) w_1 - (A_{12} - A_{22}) w_2 \rangle \\
%   &= \langle w_1 - w_2, (b_1 - b_2) + (A_{12} - A_{11}) (w_1 - w_2) + ((A_{12} - A_{11}) - (A_{12} - A_{22})) w_2 \rangle \\
% \end{align}

% Note
% \begin{align}
%   &\frac{d}{dt} \langle w_1, w_2 \rangle \\
%   &= \langle \frac{d}{dt} w_1, w_2 \rangle + \langle w_1, \frac{d}{dt} w_2 \rangle \\
%   &\propto \langle b_1 - A_{11} w_1 - A_{12} w_2, w_2 \rangle + \langle w_1, b_2 - A_{12} w_1 - A_{22} w_2 \rangle \\
%   &= \langle b_1, w_2 \rangle - \langle A_{11} w_1, w_2 \rangle - \langle A_{12} w_2, w_2 \rangle + \langle w_1, b_2 \rangle - \langle A_{12} w_1, w_1 \rangle - \langle w_1, A_{22} w_2 \rangle \\
%   &= \langle b_1, w_2 \rangle + \langle w_1, b_2 \rangle - (\langle A_{12} w_2, w_2 \rangle + \langle A_{12} w_1, w_1 \rangle) - (\langle A_{11} w_1, w_2 \rangle + \langle A_{22} w_1, w_2 \rangle) \\
%   &= \langle b_1, w_2 \rangle + \langle w_1, b_2 \rangle - (\langle A_{12} (w_1+w_2), w_1+w_2 \rangle - 2 \langle A_{12} w_1, w_2 \rangle) - (\langle A_{11} w_1, w_2 \rangle + \langle A_{22} w_1, w_2 \rangle).
% \end{align}
% If we assume $w_1(0) = w_2(0)$, then
% \begin{align}
%   \left. \frac{d}{dt} \langle w_1, w_2 \rangle \right|_{t=0}
%   &= \langle b_1, w_1 \rangle - \langle A_{11} w_1, w_1 \rangle - \langle A_{12} w_1, w_1 \rangle + \langle w_1, b_2 \rangle - \langle A_{12} w_1, w_1 \rangle - \langle w_1, A_{22} w_1 \rangle
% \end{align}


\section{Theory-driven Experiments}

\subsection{General Case}

\subsection{Single Gate}

\subsection{Redundant Gates}


\section{Next Steps}
Concerns:

Questions:

Some ideas:
\begin{enumerate}
  \item Let the gates vary during training. 
  That is, $g$ depends on $t$ and $x$.
  So, $\Sigma^{yx}$ and $\Sigma^{xx}$ will depend on $t$ as well.
  We can still solve the differential equation, and we may be able to use a single basis to diagonalize them across time (Toeplitz/circulant properties? DFT?).
\end{enumerate}


\subsection{Time-dependent Gating}
Let us now consider the case where $g$ depends on $x$ as well as $t$.
That is, $g = g(t,x)$.
Then, $\Sigma^{yx}$ and $\Sigma^{xx}$ will depend on $t$ as well.
In terms of \cref{eq:grad_flow_matrix}, $b$ and $A$ depend on $t$ rather than being fixed.
That is,
\begin{align}
  K \tau \frac{d}{dt} w &= b(t) - A(t) w. \label{eq:grad_flow_matrix_time}
\end{align}

Note that $A(t)$ is always real symmetric for each $t$.
Let us momentarily assume that it is diagonalizable in the same basis across time, that is $A(t) = P \Lambda(t) P^\top$, where $\Lambda(t)$ is diagonal.


When is $A(t)$ diagonalizable in the same basis across time?
Recall that two matrices are simultaneously diagonalize iff they commute.
Let us consider two time points, $t$ and $t'$.
\begin{align}
  A(t) A(t')
  &= \begin{bmatrix} \langle g_i(t,x) g_j(t,x) x x^\top \rangle_{x,y,g} \end{bmatrix}_{ij}
  \begin{bmatrix} \langle g_i(t,x') g_j(t,x') x' x'^\top \rangle_{x',y',g} \end{bmatrix}_{jk} \\
  &= \begin{bmatrix} \sum_l \langle g_i(t,x) g_l(t,x) x x^\top \rangle_{x,y,g} \langle g_l(t',x') g_j(t',x') x' x'^\top \rangle_{x',y',g} \end{bmatrix}_{ij} \\
  &= \begin{bmatrix} \sum_l \left\langle g_i(t,x) g_l(t,x) x x^\top g_l(t',x') g_j(t',x') x' x'^\top \right\rangle_{x,y,x',y',g} \end{bmatrix}_{ij} \\
  &= \begin{bmatrix} \left\langle g_i(t,x) g_j(t',x') \sum_l g_l(t,x) g_l(t',x') x x^\top x' x'^\top \right\rangle_{x,y,x',y',g} \end{bmatrix}_{ij}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Single step in a ReLU Network %%
\subsection{Single step in a ReLU Network}
Let's consider what happens in a single step of a network with ReLU activation.
% We will consider a network with just one neuron, since I've observed empirically that this still yields localization only when the gain is sufficiently large, and it's much easier to analyze.
We make predictions with
\begin{align}
  \hat{y}(x) &= \frac{1}{K} \sum_{k \in [K]} \text{ReLU}(\langle w_k, x \rangle).
\end{align}
We consider MSE loss,
\begin{align}
  \LL &= \frac{1}{2} \E_{X,Y} \left[ \left( \hat{y}(X) - Y \right)^2 \right].
\end{align}
The gradient flow for $w_1$ is given by
\begin{align}
  \tau \frac{d}{dt} w_1
  &= -\E_{X,Y} \left[ \left( \text{ReLU}(\langle w_1(t), X \rangle) - Y \right) \frac{\partial}{\partial w_1} \left[ \text{ReLU}(\langle w_1(t), X \rangle) \right] \right] \\
  % % &\begin{multlined} = \E_{x \mid y=1} \left[ \left( \text{ReLU}(\langle w_1(t), x \rangle) - y \right) \frac{\partial}{\partial w_1} \left[ \text{ReLU}(\langle w_1(t), x \rangle) \right] \right] \PR(y=1) \\
  % % + \E_{x \mid y=0} \left[ \left( \text{ReLU}(\langle w_1(t), x \rangle) - y \right) \frac{\partial}{\partial w_1} \left[ \text{ReLU}(\langle w_1(t), x \rangle) \right] \right] \PR(y=0) \end{multlined} \\
  % % &= \E_{x,y \mid \langle w_1(t), x \rangle > 0} \left[ \left( \text{ReLU}(\langle w_1(t), x \rangle) - y \right) x \right] \PR( \langle w_1(t), x \rangle > 0 ) \\
  % &= \E_{x,y \mid \langle w_1(t), x \rangle > 0} \left[ \left( \langle w_1(t), x \rangle - y \right) x \right] \PR( \langle w_1(t), x \rangle > 0 ) \\
  &= \big( \underbrace{ \E_{X, Y \mid \langle w_1(t), X \rangle > 0} \left[ Y X \right] }_{\equiv (I)} - \underbrace{ \E_{X, Y \mid \langle w_1(t), X \rangle > 0} \left[ \langle w_1(t), X \rangle X \right] }_{\equiv (II)} \big) \PR( \langle w_1(t), X \rangle > 0 ).
  % % &\begin{multlined} = \E_{x \mid y=1, \langle w_1(t), x \rangle > 0} \left[ \left( \text{ReLU}(\langle w_1(t), x \rangle) - y \right) \frac{\partial}{\partial w_1} \left[ \text{ReLU}(\langle w_1(t), x \rangle) \right] \right] \PR(y=1) \\
  % %   + \E_{x \mid y=0} \left[ \left( \text{ReLU}(\langle w_1(t), x \rangle) - y \right) \frac{\partial}{\partial w_1} \left[ \text{ReLU}(\langle w_1(t), x \rangle) \right] \right] \PR(y=0) \end{multlined} \\
\end{align}

Recall that $X$ is a mixture of $X \mid Y = 1$ and $X \mid Y = 0$.
So, we will compute these expectations separately.
% Note that
% \begin{align}
%   &\begin{multlined}\E_{x, y \mid \langle w_1(t), x \rangle > 0} \left[ f(x) \right] = \E_{x \mid y = 1, \langle w_1(t), x \rangle > 0} \left[ f(x) \right] \PR(y = 1 \mid \langle w_1(t), x \rangle > 0) \\
%   + \E_{x \mid y = 0, \langle w_1(t), x \rangle > 0} \left[ f(x) \right] \PR(y = 0 \mid \langle w_1(t), x \rangle > 0) \end{multlined}.
% \end{align}
Let us write $S = \langle w_1(t), X \rangle$.
Then, we can use the law of total expectation to write
\begin{align}
  \E_{X \mid Y = 1, \langle w_1(t), X \rangle > 0} \left[ f(X) \right]
  % &= \int_{\R^n} f(x) p(x \mid y = 1, S > 0) dx \\
  % &= \int_{\R^n} f(x) \frac{p(x, S > 0 \mid y = 1)}{p(S > 0 \mid y = 1)} dx \\
  % &= \int_{\R^n} f(x) \frac{\int_0^\infty p(x, S = s \mid y = 1) ds}{p(S > 0 \mid y = 1)} dx \\
  % &= \int_{\R^n} \int_0^\infty f(x) \frac{ p(x \mid S = s, y = 1) p(S = s \mid y = 1) }{ p(S > 0 \mid y = 1) } ds dx \\
  % &= \int_0^\infty \int_{\R^n} f(x) \frac{ p(x \mid S = s, y = 1) p(S = s \mid y = 1) }{ p(S > 0 \mid y = 1) } dx ds && \text{Fubini} \\
  % &= \int_0^\infty \frac{ p(S = s \mid y = 1) }{ p(S > 0 \mid y = 1) } \int_{\R^n} f(x) p(x \mid S = s, y = 1) dx ds \\
  % &= \int_0^\infty \frac{ p(S = s \mid y = 1) }{ p(S > 0 \mid y = 1) } \E_{x \mid S = s, y = 1}\left[ f(x) \right] ds \\
  &= %\frac{ 1 }{ p(S > 0 \mid y = 1) } 
  \E_{S > 0 \mid y = 1} \left[ \E_{X \mid S, y = 1}\left[ f(X) \right] \right].
\end{align}
% Let us rewrite the density:% using Bayes' rule:
% \begin{align}
%   &p(x, \langle w_1(t), x \rangle = s \mid y = 1, \langle w_1(t), x \rangle > 0) \\
%   &= p(x \mid \langle w_1(t), x \rangle = s, y = 1, \langle w_1(t), x \rangle > 0) p(x \mid \langle w_1(t), x \rangle = s, y = 1, \langle w_1(t), x \rangle > 0)
% \end{align}

So, we need to find the distribution of $X$ conditioned on $S \equiv \langle w_1(t), X \rangle = s$.
In general, this is very challenging.
We will split this into two terms, one of which disappears when $X$ is Gaussian.
Let $\Sigma$ be the covariance of $x$ (recall it has mean 0).
We write
\begin{align}
  X &= AX + S v,
\end{align}
where
\begin{align}
  v &= \frac{1}{ w_1(t)^\top \Sigma w_1(t) } \Sigma w_1(t), \\
  A &= I_n - v w_1(t)^\top.
\end{align}
Thus, $\E_{X \mid S=s}[X] = \E_{X \mid S=s}[AX] + s v$.
Our choice of $A$ and $v$ implies that $AX$ and $S$ have zero \textit{covariance} (see \href{https://math.stackexchange.com/questions/2784531/distribution-of-joint-gaussian-conditional-on-their-sum/2943590#2943590}{this post}).
When $X$ is Gaussian, this implies that $AX$ and $S$ are independent, so we'd have $\E_{X \mid S=s}[X] = A \E[X] + s v = s v$.
% We can show that $A \Sigma A^\top = A \Sigma$ (I think—see paper notes from 10/17).

With this representation,
\begin{align}
  \E_{X \mid Y=1, S>0} \left[ YX \right] 
  &= \E_{S>0 \mid Y=1} \left[ \E_{X \mid S, Y=1}[AX] + S v \right] \\
  &= \E_{X \mid Y=1, S>0} \left[ AX \right] + \frac{ \E_{S>0 \mid Y=1} \left[ S \right] }{ w_1(t)^\top \Sigma_1 w_1(t) } \Sigma_1 w_1(t), \label{eq:exp_yx} \\
  %
  \E_{X \mid Y=1, S>0} \left[ S X \right] 
  &= \E_{S>0 \mid Y=1} \left[ S \E_{X \mid S, Y=1}[AX] + S^2 v \right] \\
  &= \E_{X \mid Y=1, S>0} \left[ S AX \right] + \E_{S>0 \mid Y=1} \left[ S^2 \right] v \\
  &= \E_{X \mid Y=1, S>0} \left[ S AX \right] + w_1(t)^\top \Sigma_1 w_1(t) v, \\
  &= \E_{X \mid Y=1, S>0} \left[ S AX \right] + \Sigma_1 w_1(t). \label{eq:exp_sx}
\end{align}
Note that if $X$ were Gaussian, then the first terms in \cref{eq:exp_sx,eq:exp_yx} would be zero.

Now, we evaluate (I) and (II).
\begin{align}
  (I) &= \E_{X \mid S>0} \left[ S AX \right] + \left[ \PR(Y=1 \mid S>0) \Sigma_1 + \PR(Y=0 \mid S>0) \Sigma_0 \right] w_1(t), \\
  (II) &= \E_{X \mid S>0} \left[ AX \right] + \PR(Y=1 \mid S>0) \frac{ \E_{S>0 \mid Y=1} \left[ S \right] }{ w_1(t)^\top \Sigma_1 w_1(t) } \Sigma_1 w_1(t)
\end{align}

Then,
\begin{align}
  &\left[ (I) - (II) \right] \PR( S > 0 ) \\
  &\begin{multlined} = -\E_{X \mid S>0} \left[ (S-1) AX \right]  \\
  - \left[ \PR(Y=1 \mid S>0) \left( \frac{ \E_{S>0 \mid Y=1} \left[ S \right] }{ w_1(t)^\top \Sigma_1 w_1(t) } - 1 \right) \Sigma_1 + \PR(Y=0 \mid S>0) \frac{ \E_{S>0 \mid Y=0} \left[ S \right] }{ w_1(t)^\top \Sigma_0 w_1(t) } \Sigma_0 \right] w_1(t).
  % + \left[ \frac{ \E_{S>0, Y=1} \left[ S \right] - 1 }{ w_1(t)^\top \Sigma_1 w_1(t) } \Sigma_1 + \frac{ \E_{S>0, Y=0} \left[ S \right] - 1 }{ w_1(t)^\top \Sigma_0 w_1(t) } \Sigma_0 \right] w_1(t)
  \end{multlined}
\end{align}
By symmetry, $\PR(Y=1 \mid S>0) = \frac{1}{2}$ and $PR(S > 0) = \frac{1}{2}$.
Then,
\begin{align}
  4 \tau \frac{d}{dt} w_1
  &= -\E_{X \mid S>0} \left[ (S-1) AX \right] - \left[ \left( \frac{ \E_{S>0 \mid Y=1} \left[ S \right] }{ w_1(t)^\top \Sigma_1 w_1(t) } - 1 \right) \Sigma_1 + \frac{ \E_{S>0 \mid Y=0} \left[ S \right] }{ w_1(t)^\top \Sigma_0 w_1(t) } \Sigma_0 \right] w_1(t)
\end{align}
Also recall that $\Sigma_1$ and $\Sigma_0$ both diagonalize in the discrete Fourier basis, which we denote with $P$, and their corresponding diagonal matrices of eigenvalues $\Lambda_1$ and $\Lambda_0$.
Write $u_1 = P^\top w_1$.
\begin{align}
  4 \tau \frac{d}{dt} u_1
  &= -\E_{X \mid S>0} \left[ (S-1) P^\top AX \right] - \left[ \left( \frac{ \E_{S>0 \mid Y=1} \left[ S \right] }{ u_1(t)^\top \Lambda_1 u_1(t) } - 1 \right) \Lambda_1 + \frac{ \E_{S>0 \mid Y=0} \left[ S \right] }{ u_1(t)^\top \Lambda_0 u_1(t) } \Lambda_0 \right] u_1(t).
\end{align}

Let us expand the first term for $Y=1$.
Define $\Xi = P^\top X$.
\begin{align}
  \E_{X \mid Y=1, S>0} \left[ (S-1) P^\top AX \right]
  &= \E_{X \mid Y=1, S>0} \left[ (S-1) P^\top \left( I_n - \frac{\Sigma_1 w_1(t) w_1(t)^\top}{w_1(t)^\top \Sigma_1 w_1(t)} \right) X \right] \\
  &= \E_{\Xi \mid Y=1, \langle u_1(t), \Xi \rangle > 0} \left[ (\langle u_1(t), \Xi \rangle-1) \left( I_n - \frac{\Lambda_1 u_1(t) u_1(t)^\top}{u_1(t)^\top \Lambda_1 u_1(t)} \right) \Xi \right].
\end{align}
Now, we must stop and ask: what is $\Xi$?
Recall that $P$ is the discrete Fourier basis.
Importantly, it is actually the \textit{real} part of the discrete Fourier basis since $\Sigma_1$ is symmetric.
That is, with $\omega = e^{-\frac{2 \pi i}{n}}$,
\begin{align}
  P_{:,j} 
  &= \Re \frac{1}{\sqrt{n}} \begin{bmatrix} 1 \\ \omega^j \\ \omega^{2j} \\ \vdots \\ \omega^{(n-1)j} \end{bmatrix}
  = \frac{1}{\sqrt{n}} \begin{bmatrix} 1 \\ \cos( \frac{2 \pi}{n} j ) \\ \cos( \frac{2 \pi}{n} 2j ) \\ \vdots \\ \cos( \frac{2 \pi}{n} (n-1) j ) \end{bmatrix}.
\end{align}

$\Xi$ is the discrete Fourier transform (DFT) of $X$.

TRY COMPUTING DENSITY OF NON-GAUSSIAN USING TRICK; DON'T THINK FOURIER APPROACH WILL BE VERY HELPFUL TBH.





\subsubsection{Gaussian $X$}
Now, let us assume that $X$ is Gaussian.
Then, $\E_{X \mid S>0} \left[ (S-1) P^\top AX \right] = 0$.
Furthermore,
\begin{align}
  \E_{S>0 \mid Y=1} \left[ S \right] 
  &= \sqrt{ \frac{2}{\pi} } \left( w_1(t)^\top \Sigma_1 w_1(t) \right)^{\frac{1}{2}}
  = \sqrt{ \frac{2}{\pi} } \left( u_1(t)^\top \Lambda_1 u_1(t) \right)^{\frac{1}{2}}.
\end{align}
Then, the gradient flow becomes
\begin{align}
  4 \tau \frac{d}{dt} u_1
  &= -\left[ \left( \frac{ 1 }{ \sqrt{ u_1(t)^\top \Lambda_1 u_1(t) } } - 1 \right) \Lambda_1 + \frac{ 1 }{ \sqrt{ u_1(t)^\top \Lambda_0 u_1(t) } } \Lambda_0 \right] u_1(t).
\end{align}





\newpage
So,
\begin{align}
  \E_{S > 0 \mid y = 1} \left[ \E_{x \mid S, y = 1}\left[ x \right] \right]
  &= \E_{S > 0 \mid y = 1} \left[ s v_1 \right] 
  = \E_{S > 0 \mid y = 1} \left[ s \right] v_1, \\
  \E_{S > 0 \mid y = 1} \left[ \E_{x \mid S, y = 1}\left[ x x^\top \right] \right]
  &= \E_{S > 0 \mid y = 1} \left[ A_1 \Sigma_1 A_1^\top \right]
  = A_1 \Sigma_1 A_1^\top.
\end{align}
Note that $S \sim \NN\left( 0, w_1(t)^\top \Sigma w_1(t) \right)$.
So, $\E_{S > 0 \mid y = 1} \left[ s \right] = \left( \frac{2}{\pi} w_1(t)^\top \Sigma w_1(t) \right)^{\frac{1}{2}}$.
In summary,
\begin{align}
  \E_{x \mid y = 1, \langle w_1(t), x \rangle > 0} \left[ x \right]
  &= \frac{ \left( \frac{2}{\pi} w_1(t)^\top \Sigma w_1(t) \right)^{\frac{1}{2}} }{ w_1(t)^\top \Sigma w_1(t) } \Sigma w_1(t)
  = \sqrt{ \frac{2}{\pi} } \left( w_1(t)^\top \Sigma w_1(t) \right)^{-\frac{1}{2}} \Sigma w_1(t), \\
  \E_{x \mid y = 1, \langle w_1(t), x \rangle > 0} \left[ x x^\top \right]
  &= A_1 \Sigma_1 A_1^\top
  = \left( I_n - \frac{1}{w_1(t)^\top \Sigma_1 w_1(t)} \Sigma_1 w_1(t) w_1(t)^\top \right) \Sigma_1.
\end{align}

Rewriting the gradient flow,
\begin{align}
  &\frac{\tau}{\PR( \langle w_1(t), x \rangle > 0 )} \frac{d}{dt} w_1 \\
  & \begin{multlined} =\left[ \left( I_n - \frac{1}{w_1(t)^\top \Sigma_1 w_1(t)} \Sigma_1 w_1(t) w_1(t)^\top \right) \Sigma_1 + \left( I_n - \frac{1}{w_1(t)^\top \Sigma_0 w_1(t)} \Sigma_0 w_1(t) w_1(t)^\top \right) \Sigma_0 \right] w_1(t) \\
  - \left[ \sqrt{ \frac{2}{\pi} } \left( w_1(t)^\top \Sigma w_1(t) \right)^{-\frac{1}{2}} \Sigma w_1(t) \right] \end{multlined} \\
  &= %\left( \Sigma_1 w_1(t) - \Sigma_1 w_1(t) \right) + \left( \Sigma_0 w_1(t) - \Sigma_0 w_1(t) \right) 
    - \sqrt{ \frac{2}{\pi} } \left( w_1(t)^\top \Sigma_1 w_1(t) \right)^{-\frac{1}{2}} \Sigma_1 w_1(t).
\end{align}
By symmetry, $\PR( \langle w_1(t), x \rangle > 0 ) = \frac{1}{2}$.
\begin{align}
  \tau \frac{d}{dt} w_1
  &= - 2 \sqrt{ \frac{2}{\pi} } \left[ w_1(t)^\top \Sigma_1 w_1(t) \right]^{-\frac{1}{2}} \Sigma_1 w_1(t).
\end{align}
Let us write $\Sigma_1 = P \Lambda P^\top$, where $\Lambda$ is diagonal and $P$ is orthogonal, and $u_1 = P^\top w_1$.
Then,
\begin{align}
  \tau \frac{d}{dt} u_1
  &= - 2 \sqrt{ \frac{2}{\pi} } \left[ u_1(t)^\top \Lambda u_1(t) \right]^{-\frac{1}{2}} \Lambda u_1(t). \label{eq:grad_flow_diag}
\end{align}
So it appears that this shrinks $u_1$ to 0, and this is accelerated as $u_1$ gets small by the weighted norm.
This is consistent with what we observe for the Gaussian case.
But what happens when $x$ is non-Gaussian.
Analytically, it's hard to say exactly.
But it seems like the input-input covariance terms will \emph{not} cancel, and so we will have another term that hopefully does more than just shrink $u_1$ to 0.
It is surprising that $w_1(t)$ pops out of the input-output term.
This \emph{does not happen in the gated linear network}, and this seems like a key difference.
What does this say about gating early during training?
I should double-check this to make sure it's right.
If this weren't the case though, we would get a bias term that probably yields localization (or some nonzero structure).

What we see in \cref{eq:grad_flow_diag} is that the largest eigenvalues are shrunk fastest.
This corresponds to the longest-frequency signals disappearing quickly.
So, we see that the Gaussian noise quickly turns into a short-range oscillation, which is then damped out to 0.
This seems consistent with my ReLU simulations, but I need to make sure the results are perfectly comparable (same learning rate, using exact same data at each time step, etc.)



% Note that
% \begin{align}
%   \frac{\tau}{2} \frac{d}{dt} \langle u_1, u_1 \rangle
%   &= \langle u_1, \tau \frac{d}{dt} u_1 \rangle \\
%   &= - 2 \sqrt{ \frac{2}{\pi} } \left[ u_1(t)^\top \Lambda u_1(t) \right]^{-\frac{1}{2}} \langle u_1, \Lambda u_1(t) \rangle \\
%   &= - 2 \sqrt{ \frac{2}{\pi} } \left[ u_1(t)^\top \Lambda u_1(t) \right]^{\frac{1}{2}}.
% \end{align}



% % So, we can write the first term in \cref{eq:lotl_xx} as
% % \begin{align}
% %   &\E_{S} \left[ \E_{x \mid \langle w_1(t), x \rangle = S, y = 1} \left[ x x^\top \right] \PR\left( y = 1 \mid \langle w_1(t), x \rangle = S \right) \right] \\
% %   &= \E_S \left[ A_1 \Sigma_1 \PR\left( y = 1 \mid \langle w_1(t), x \rangle = S \right) \right] \\
% %   &= A_1 \Sigma_1 \PR\left( y = 1 \mid \langle w_1(t), x \rangle > 0 \right).
% % \end{align}
% Here, $\Sigma_1$ denotes the covariance for $x \mid y = 1$ and $A_1$ denotes the corresponding matrix $A$.
% We can do the same for the second term in \cref{eq:lotl_xx} corresponding to $y = 0$, writing it in terms of $\Sigma_0$ and $A_0$.
% Thus,
% \begin{align}
%   &\E_{x \mid \langle w_1(t), x \rangle > 0} \left[ x x^\top \right] \\
%   % &= A_1 \Sigma_1 \PR\left( y = 1 \mid \langle w_1(t), x \rangle > 0 \right) \PR\left( \langle w_1(t), x \rangle > 0 \right) + A_0 \Sigma_0 \PR\left( y = 0 \mid \langle w_1(t), x \rangle > 0 \right) \PR\left( \langle w_1(t), x \rangle > 0 \right) \\
%   &= A_1 \Sigma_1 \PR\left( \langle w_1(t), x \rangle > 0 \mid y = 1 \right) \PR(y = 1) + A_0 \Sigma_0 \PR\left( \langle w_1(t), x \rangle > 0 \mid y = 0 \right) \PR( y = 0 ).
% \end{align}
% Let us evaluate these conditional probabilities.
% Again, recall that $x \mid y=1 \sim \NN\left( 0, \Sigma_1 \right)$.
% We can immediately recognize that $\langle w_1(t), x \rangle$ is a one-dimensional Gaussian with mean zero.
% Thus, the conditional probability is $\frac{1}{2}$, for both $y=1$ and $y=0$.
% So,
% \begin{align}
%   &\E_{x \mid \langle w_1(t), x \rangle > 0} \left[ x x^\top \right] \\
%   &= \frac{1}{2} \left( A_1 \Sigma_1 \PR(y = 1) + A_0 \Sigma_0 \PR( y = 0 ) \right) \\
%   &= \frac{1}{4} \left( A_1 \Sigma_1 + A_0 \Sigma_0 \right),
% \end{align}
% where in the final step we assume that the classes are balanced, so $\PR(y=1) = \PR(y=0) = \frac{1}{2}$.

% We have successfully computed the input-input covariance matrix for Gaussian $x$.
% Now, let us compute the input-output covariance matrix.
% We use the same approach,
% \begin{align}
%   \E_{x \mid \langle w_1(t), x \rangle > 0} \left[ y x \right]
%   &= \E_{S} \left[ \E_{x \mid \langle w_1(t), x \rangle = S} \left[ y x \right] \right] \\
%   &= \E_{S} \left[ \E_{x \mid \langle w_1(t), x \rangle = S, y = 1} \left[ x \right] \PR(y = 1 \mid \langle w_1(t), x \rangle = S) \right] \\
%   &= \E_{S} \left[ S v_1 \PR(y = 1 \mid \langle w_1(t), x \rangle = S) \right] && \text{from above} \\
%   &= \E_{S} \left[ S \PR(y = 1 \mid \langle w_1(t), x \rangle = S) \right] v_1.
% \end{align}




\end{document}
