% GO TO: /Users/leonlufkin/Library/texmf/tex/latex
%        to add custom packages to the path
\documentclass{article}
\usepackage{report}

\title{Receptive Field Localization}
\author{Leon Lufkin}
\date{\today}

\makeatletter
\let\Title\@title
\let\Author\@author
\let\Date\@date

\begin{document}

%%%%%%%%%%%
%% Model %%
%%%%%%%%%%%
\section{Model}
We begin with a general model that encapsulates many of the works we will discuss.
For an input $\mathbf{x}$, we want to predict a response $\mathbf{y}^*$.
Our prediction is a function $f_\theta$ of $\mathbf{x}$.
The function $f_\theta$ is parameterized by $\theta$.
We will consider models of the form
\begin{equation}
  \label{eq:general_model}
  f_\theta(\mathbf{x}) = y_\theta \left( \sum_i \alpha_\theta^{(i)} \phi_\theta^{(i)} (\mathbf{x}) \right).
\end{equation}
Here, $\mathbf{x}$ is an $L$-dimensional real vector and $\mathbf{y}^*$ is an $M$-dimensional real vector.
The $\phi_i$ are basis functions that map from $\R^L$ to $\R^K$, and the $\alpha_i$ are real scalars.
Additionally, the sum over $i$ need not be finite.
The coefficients $\alpha_i$ may depend on $\mathbf{x}$ and our parameters $\theta$, but we suppress the former dependence for notational simplicity. 
The function $y_\theta$ maps from $\R^K$ to $\R^M$ and is parameterized by $\theta$ as well.

We will consider a penalized mean squared error loss function
\begin{align}
  \LL(\theta) &= \frac{1}{2} \E_{\mathbf{x}, \mathbf{y}^*} \left[  \norm{ \mathbf{y}^* - f_\theta(\mathbf{x}) }_2^2  \right] + \lambda p(\{ \alpha^{(i)} \}_i, \theta).
\end{align}
Our penalty function $p$ serves to induce sparseness in the coefficients $\alpha_i$, and perhaps also regularize the parameters $\theta$.

We will show how this model encapsulates the works we are interested in understanding.

%%%%%%%%%%%%%%
%% Ingrosso %%
%%%%%%%%%%%%%%
\section{Ingrosso et al. (2022)}
The model in Ingrosso et al. (2022) sets $M = 1$, uses linear basis functions, and sets $y_\theta$ to be the mean function after applying a nonlinearity.
That is,
\begin{align}
  \sum_i \alpha_\theta^{(i)} \phi_\theta^{(i)} (\mathbf{x}) &= \Theta \mathbf{x} + b_\theta \\
  y_\theta(\mathbf{x}) &= \frac{1}{K} \bm{1}^\top \sigma(\mathbf{x}).
\end{align}
Here, $\Theta$ is a $K \times L$ matrix, $b_\theta$ is a $K$-dimensional vector, and $\sigma$ is a nonlinear function applied elementwise.
It seems that in much of their work, $b_\theta$ is fixed at $-1$ or $0$. % FIXME: when is it fixed at -1?
\hl{I still need to confirm experimentally that this does not affect the results.}

In this work, we also ignore the penalty term.
I believe that our gradient update is of the form
\begin{align}
  a
\end{align}


%%%%%%%%%%%%%%%%
%% DATA MODEL %%
%%%%%%%%%%%%%%%%
\section{Data Model}
We assume that our data is generated by the following model.
It is paramaterized by the length of the input, $L \in \N$.
It is also parameterized by a scale parameter $\xi \leq L$.
We construct data $\{ X_l \}_l \subseteq \{ 0,1 \}^L$ as follows:
\begin{enumerate}
  \item Sample integers $l^* \sim \text{Uniform}[1, L]$ (starting position) and $T \sim \text{Uniform}[1, \xi)$ (length of pulse).
  \item For $0 \leq i \leq T$, set $X_{l^* + i \pmod{L}} = 1$, and set all other $X_l$ to $0$.
  \item Return the sequence $\{ X_l \}_l$.
\end{enumerate}
Now, we derive the conditional probability $p_{11} \triangleq \PR( X_a = 1 \mid X_b = 1 )$.
For now, assume $d \triangleq b - a > 0$.
If $d \geq \xi$, then $p_{11} = 0$.
So, assume $d < \xi$.

Now, we count the number of values of $T$ that result in both $X_a$ and $X_b$ being in the pulse for a given $l^*$.
WLOG, assume $a = 0$.
For $1 \leq l^* \leq d$, the range of values for $T$ that results in both $X_a$ and $X_b$ being in the pulse is given by
\begin{align}
  L - l^* \leq T < \xi. \label{eq:case1_Trange}
\end{align}
Thus, the number of values for $T$ in this case is $\max( \xi - L + l^* , 0 )$.

For $d < l^* \leq L$, the values of $T$ that result in both $X_a$ and $X_b$ being in the pulse is given by
\begin{align}
  L - (l^* - d) \leq T < \xi. \label{eq:case2_Trange}
\end{align}
So, the number of values for $T$ in this case is $\max(\xi - (L - (l^* - d)), 0) = \max(\xi - L + l^* - d, 0)$.

Note that the first max condition is at least zero when $l^* \geq L - \xi$.
This yields a sum over $\max(L - \xi, 1) \leq l^* \leq d$.
Similarly, the second max condition is at least zero when $l^* \geq L - \xi + d$.
This yields a sum over $\max( L - \xi + d, d+1) \leq l^* \leq L$.
Note that the first term dominates in both of these new max statements when $\xi \leq L-1$.
So, let us assume this is the case.

Now, we want to find the total number of values of $T$ that result in both $X_a$ and $X_b$ being in the pulse.
\begin{align}
  T_1 
  &= \sum_{ L - \xi \leq l^* \leq d } (\xi - L + l^*) \label{eq:T1} \\
  T_2
  &= \sum_{ L - \xi + d \leq l^* \leq L } (\xi - L + l^* - d)
  = \sum_{ L - \xi \leq t \leq L - d } (\xi - L + t). \label{eq:T2}
\end{align}
% Let us assume $d \geq L - \xi$. % \iff \xi \geq L - d$.
We compute $T_1$ as,
% \begin{align*}
%   T_1
%   &= (\xi - L) (d - (L-\xi) + 1) + \frac{d(d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} \\
%   &= d \xi - L \xi + \frac{\xi^2}{2} + \frac{3\xi}{2} - Ld + \frac{L^2}{2} - \frac{3L}{2} + \frac{3d}{2} + 1 + \frac{d^2}{2} - d + L - \xi - 1 \\
%   &= d \xi - L \xi + \frac{\xi^2}{2} + \frac{\xi}{2} - Ld + \frac{L^2}{2} - \frac{L}{2} + \frac{d}{2} + \frac{d^2}{2}.
% \end{align*}
\begin{align*}
  T_1
  % &= (\xi - L) (d - (L-\xi) + 1) + \frac{d(d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} \\
  &= \left[ (\xi - L) (d - L + \xi + 1) + \frac{d(d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} \right] \mathbbm{1}(d \geq L - \xi).
\end{align*}
% Let us further assume $L - \xi + d \leq L \iff d \leq \xi$.
Next, we compute $T_2$ as,
% \begin{align*}
%   T_2
%   &= (L - (L-\xi+d) + 1) (\xi - L - d) + \frac{L(L+1)}{2} - \frac{(L-\xi+d)(L-\xi+d-1)}{2} \\
%   &= \frac{\xi^2}{2} - d\xi + \frac{\xi}{2} + \frac{d^2}{2} - \frac{d}{2}.
% \end{align*}
\begin{align*}
  T_2
  % &= ((L - d) - (L - \xi) + 1) (\xi - L) + \frac{(L-d)(L-d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} \\
  &= \left[ (\xi - d + 1) (\xi - L) + \frac{(L-d)(L-d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} \right] \mathbbm{1}(d \leq \xi).
\end{align*}
Thus, the total number of values of $T$ that result in both $X_a$ and $X_b$ being in the pulse is
% \begin{align}
%   T_1 + T_2
%   &= d \xi - L \xi + \frac{\xi^2}{2} + \frac{\xi}{2} - Ld + \frac{L^2}{2} - \frac{L}{2} + \frac{d}{2} + \frac{d^2}{2}
%   + \frac{\xi^2}{2} - d\xi + \frac{\xi}{2} + \frac{d^2}{2} - \frac{d}{2} \\
%   &= -L\xi + \xi^2 + \xi - Ld + \frac{L^2}{2} - \frac{L}{2} + d^2.
% \end{align}
\begin{align*}
  T_1 + T_2
  % &= (\xi - L) [ (d - L + \xi + 1) + (\xi - d + 1) ] + \frac{d(d+1)}{2} + \frac{(L-d)(L-d+1)}{2} - (L-\xi)(L-\xi-1) \\
  % &= (L - \xi) [(L-\xi-1) - (2\xi + 2- L)] + \frac{d(d+1)}{2} + \frac{(L-d)(L-d+1)}{2} \\
  % &= (L - \xi) (2L + \xi - 3) + \frac{d(d+1)}{2} + \frac{(L-d)(L-d+1)}{2} \\
  &= 
  \begin{cases}
    (\xi - d + 1) (\xi - L) + \frac{(L-d)(L-d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} & d \leq \xi, L - \xi + 1 \\
    d^2 - dL + \frac{L^2}{2} + \frac{L}{2} - (L-\xi) (\xi+1) & L - \xi \leq d \leq \xi \\
    (\xi - L) (d - L + \xi + 1) + \frac{d(d+1)}{2} - \frac{(L-\xi)(L-\xi-1)}{2} & d \geq L - \xi, \xi + 1
  \end{cases}
\end{align*}
There are $\xi - 1$ possible values for $T$ and $L$ values for $l^*$.
Recall we sample $T$ and $l^*$ uniformly and independently.
Thus, the probability that $X_b = 1$ given $X_a = 1$ is
\begin{align}
  p_{11}
  &= \frac{T_1 + T_2}{(\xi - 1) L}
  % = \frac{ -L\xi + \xi^2 + \xi - Ld + \frac{L^2}{2} - \frac{L}{2} + d^2 }{ (\xi - 1) L }.
  = \frac{ d^2 - dL + \frac{L^2}{2} + \frac{L}{2} - (L-\xi) (\xi+1) }{ (\xi - 1) L }.
\end{align}
Note that we assumed $\xi \leq L-1$.
However, we can check that the above expression is still valid when $\xi = L$.
(Check Desmos. 
Otherwise, an exercise left to the reader.)
It also satisfies the ``sanity check'' that it is minimized at $x = \frac{L}{2}$.


\section{Nonlinear Gaussian Process}
We now consider the nonlinear Gaussian process (NLGP) data model.
Define a covariance matrix $C$ with entries $C_{ij} = \exp(-(i-j)^2/\xi^2)$ for $i,j \in [L]$.
Let $Z = [Z_1, \ldots, Z_L] \sim \mathcal{N}(0, C)$.
Then, $Z$ is a Gaussian process with covariance $C$.
Consider a nonlinearity $\psi$.
We specifically consider the error function, $\psi(z) = \text{erf}(z / \sqrt{2})$.
Define $X_i = \psi(g Z_i)$ for $i \in [L]$ and some $g > 0$.
Then, $X$ is a nonlinear Gaussian process (NLGP).

Now, we will compute the moments of $X$.
Each $X_i$ clearly has mean zero.
To compute higher moments, we will need to employ some handy properties of the error function.
Assume $Z_1$ and $Z_2$ have covariance $\rho$.
First, note
\begin{align}
  \E[ X_1 X_2 ]
  &= \E_{X_1}\left[ \E_{X_2}\left[ X_1 X_2 \mid X_1 \right] \right] && \text{Law of Total Expectation} \\
  &= \E_{X_1}\left[ X_1 \E_{X_2}\left[ X_2 \mid Z_1 \right] \right]. && \text{$\psi$ invertible}
\end{align}
Now, we compute the inner expectation.
\begin{align}
  \E_{X_2}\left[ X_2 \mid Z_1 \right]
  &= \E_{Z_2}\left[ \psi(g Z_2) \mid Z_1 \right] \\
  &= \int_{-\infty}^{\infty} \text{erf}\left( \frac{g z}{\sqrt{2}} \right) \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{1}{2 \sigma^2} \left( z - \mu \right)^2} dz && \mu = \rho Z_1, \sigma^2 = 1 - \rho^2 \\
  &= \text{erf}\left( \frac{ g \mu / \sqrt{2} }{ \sqrt{ 1 + g^2 \sigma^2 } } \right) \\
  &= \text{erf}\left( \frac{ \rho g }{ \sqrt{ 2( 1 + g^2 (1 - \rho^2) ) } } Z_1 \right).
  % = \text{erf} \left( \frac{ \sqrt{\rho} Z_1 / \sqrt{2 g} }{ \sqrt{ 1 + 2(1-\rho^2) } } \right). && \text{Wikipedia-assisted}
\end{align}
Now, the outer expectation
\begin{align}
  &\E_{X_1} \left[  X_1 \text{erf}\left( \frac{ \rho g }{ \sqrt{ 2( 1 + g^2 (1 - \rho^2) ) } } Z_1 \right) \right] \\
  &= \E_{Z_1} \left[ \text{erf} \left( \frac{g Z_1}{\sqrt{2}} \right) \text{erf}\left( \frac{ \rho g }{ \sqrt{ 2( 1 + g^2 (1 - \rho^2) ) } } Z_1 \right) \right] \\
  &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} z^2} \text{erf} \left( \frac{g}{\sqrt{2}} z \right) \text{erf}\left( \frac{ \rho g }{ \sqrt{ 2( 1 + g^2 (1 - \rho^2) ) } } z \right) dz \\
  &= \frac{2}{\sqrt{2 \pi}} \int_{0}^{\infty} e^{-\frac{1}{2} z^2} \text{erf} \left( \frac{g}{\sqrt{2}} z \right) \text{erf}\left( \frac{ \rho g }{ \sqrt{ 2( 1 + g^2 (1 - \rho^2) ) } } z \right) dz && \text{integrand is even} \\
  &= \frac{2}{\pi} \tan^{-1} \left( \frac{ \frac{g}{\sqrt{2}} \cdot \frac{ \rho g }{ \sqrt{ 2( 1 + g^2 (1 - \rho^2) ) } } }{ \sqrt{ \frac{1}{2} \left( \frac{g^2}{2} + \frac{ \rho^2 g^2 }{ 2( 1 + g^2 (1 - \rho^2) ) } + \frac{1}{2} \right) } } \right) && \text{Prudnikov et al. (1990)} \\
  % &= \frac{2}{\pi} \tan^{-1} \left( \frac{ \frac{ \rho g^2 }{ 2 \sqrt{ 1 + g^2 (1 - \rho^2) } } }{ \sqrt{ \frac{1}{2} \left( \frac{g^2}{2} + \frac{ \rho^2 g^2 }{ 2( 1 + g^2 (1 - \rho^2) ) } + \frac{1}{2} \right) } } \right)
  &= \frac{2}{\pi}\sin^{-1}\left(\frac{g^{2}}{1+g^{2}}\rho\right). && \text{I promise}
\end{align}

We use the same idea to compute a third-order moment,
\begin{align}
  \E[ X_1 X_2 X_3 ]
  &= \E_{X_1}\left[ \E_{X_2} \left[ \E_{X_3}\left[ X_1 X_2 X_3 \mid X_2 \right] \mid X_2 \right] \right] && \text{Law of Total Expectation} \\
  &= \E_{X_1}\left[  X_1 \E_{X_2} \left[ X_2 \E_{X_3}\left[ X_3 \mid X_2 \right] \mid X_2 \right] \right].
\end{align}




\end{document}
