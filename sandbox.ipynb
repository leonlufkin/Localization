{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# from nets import datasets\n",
    "from datasets.nonlinear_gp import NonlinearGPDataset\n",
    "from nets.launch import configs\n",
    "from nets.models.feedforward import MLP\n",
    "from models.feedforward import SimpleNet\n",
    "from nets import samplers\n",
    "# from nets.simulators.online_sgd import train_step, eval_step, evaluate, simulate\n",
    "from experiments.online_sgd import simulate\n",
    "# from nets.experiments.analyzable_online_sgd.launcher_local import SearchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "from nets.launch.hparams import Param\n",
    "from nets.launch.hparams import EnumParam\n",
    "from nets.launch.hparams import FixedParam\n",
    "\n",
    "@dataclass(frozen=True, kw_only=True)\n",
    "class SearchConfig(configs.Config):\n",
    "  \"\"\"Generic config for a hyperparameter search.\"\"\"\n",
    "\n",
    "  seed: Param = field(default_factory=lambda: FixedParam(0))\n",
    "\n",
    "  # Model params.\n",
    "  # num_ins: Param = field(default_factory=lambda: FixedParam(4))\n",
    "  num_hiddens: Param = field(default_factory=lambda: FixedParam(100))\n",
    "  init_scale: Param = field(default_factory=lambda: FixedParam(0.01))\n",
    "  num_dimensions: Param = field(default_factory=lambda: FixedParam(40))\n",
    "\n",
    "  # Training and evaluation params.\n",
    "  optimizer_fn: Param = field(default_factory=lambda: FixedParam(optax.adam))\n",
    "  learning_rate: Param = field(default_factory=lambda: FixedParam(1e-3))\n",
    "  batch_size: Param = field(default_factory=lambda: FixedParam(100))\n",
    "  num_epochs: Param = field(default_factory=lambda: FixedParam(1))\n",
    "\n",
    "  # Dataset params.\n",
    "  dataset_cls: Param = field(default_factory=lambda: FixedParam(NonlinearGPDataset))\n",
    "  xi1: Param = field(default_factory=lambda: FixedParam(0.1))\n",
    "  xi2: Param = field(default_factory=lambda: FixedParam(1.1))\n",
    "  gain: Param = field(default_factory=lambda: FixedParam(1.0))\n",
    "  # num_dimensions: Param = field(default_factory=lambda: FixedParam(4))\n",
    "  sampler_cls: Param = field(default_factory=lambda: FixedParam(samplers.EpochSampler))\n",
    "  \n",
    "#   dataset_cls: Param = field(init=False)\n",
    "#   num_dimensions: Param = field(init=False)\n",
    "#   num_exemplars_per_class: Param = field(init=False)\n",
    "#   exemplar_noise_scale: Param = field(init=False)\n",
    "\n",
    "  # Sampler params.\n",
    "#   sampler_cls: Param = field(init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend: cpu\n",
      "\n",
      "Using configuration:\n",
      "{'batch_size': 100,\n",
      " 'dataset_cls': <class 'datasets.nonlinear_gp.NonlinearGPDataset'>,\n",
      " 'gain': 1.0,\n",
      " 'init_scale': 0.01,\n",
      " 'learning_rate': 0.001,\n",
      " 'num_dimensions': 40,\n",
      " 'num_epochs': 1,\n",
      " 'num_hiddens': 100,\n",
      " 'optimizer_fn': <function adam at 0x13b186fc0>,\n",
      " 'sampler_cls': <class 'nets.samplers.base.EpochSampler'>,\n",
      " 'seed': 0,\n",
      " 'xi1': 0.1,\n",
      " 'xi2': 1.1}\n",
      "\n",
      "simulate: len(dataset)=1000\n",
      "Model:\n",
      "SimpleNet(\n",
      "  fc1=Linear(\n",
      "    weight=f32[100,40],\n",
      "    bias=f32[100],\n",
      "    in_features=40,\n",
      "    out_features=100,\n",
      "    use_bias=True\n",
      "  ),\n",
      "  act=<wrapped function <lambda>>\n",
      ")\n",
      "\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation over 1000 examples in 1.35 secs.\n",
      "####\n",
      "ITERATION 0\n",
      "eval set:\n",
      "\n",
      "\tloss:\t\t\t1.0002896785736084\n",
      "\taccuracy:\t\t0.0\n",
      "\tBASELINE:\t\t50.00%\n",
      "\tGT labels:\t\t[ 1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1.\n",
      " -1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1. -1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.\n",
      "  1.  1.  1. -1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.\n",
      " -1.  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      " -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1.  1. -1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1. -1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      " -1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.\n",
      " -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.\n",
      "  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1.\n",
      "  1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
      " -1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1.\n",
      " -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1.  1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.\n",
      "  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.\n",
      " -1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.]\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape float32[].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m search_config \u001b[39m=\u001b[39m SearchConfig(key\u001b[39m=\u001b[39mjax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(\u001b[39m0\u001b[39m), num_configs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m params \u001b[39m=\u001b[39m search_config[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m simulate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/experiments/online_sgd.py:351\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(seed, num_hiddens, init_scale, optimizer_fn, learning_rate, batch_size, num_epochs, dataset_cls, xi1, xi2, gain, num_dimensions, sampler_cls)\u001b[0m\n\u001b[1;32m    349\u001b[0m (train_key,) \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(train_key, \u001b[39m1\u001b[39m)\n\u001b[1;32m    350\u001b[0m train_step_num \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mnext\u001b[39m(itercount))\n\u001b[0;32m--> 351\u001b[0m train_loss, model, opt_state \u001b[39m=\u001b[39m train_step(\n\u001b[1;32m    352\u001b[0m   model, optimizer, opt_state, x, y, train_key\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m train_step_num \u001b[39m%\u001b[39m evaluation_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(sampler):\n\u001b[1;32m    356\u001b[0m   metrics\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    357\u001b[0m     evaluate(\n\u001b[1;32m    358\u001b[0m       iteration\u001b[39m=\u001b[39mtrain_step_num,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     )\n\u001b[1;32m    365\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/equinox/_jit.py:107\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39mFalse\u001b[39;49;00m, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/equinox/_jit.py:103\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached(dynamic, static)\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached(dynamic, static)\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m _postprocess(out)\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/experiments/online_sgd.py:75\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, opt_state, x, y, key)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@eqx\u001b[39m\u001b[39m.\u001b[39mfilter_jit\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\n\u001b[1;32m     67\u001b[0m   model: eqx\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m   key: KeyArray,\n\u001b[1;32m     73\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Array, eqx\u001b[39m.\u001b[39mModule, Array]:\n\u001b[1;32m     74\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Train the model on a single example.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m   loss, grads \u001b[39m=\u001b[39m compute_loss(model, x, y, key)\n\u001b[1;32m     76\u001b[0m   \u001b[39m# print(\"grads l2 norm: \", jnp.linalg.norm(grads(x, key=key)))\u001b[39;00m\n\u001b[1;32m     77\u001b[0m   \u001b[39m# print(grads)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m   updates, opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/experiments/online_sgd.py:61\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(model, x, y, key)\u001b[0m\n\u001b[1;32m     59\u001b[0m pred_y \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(model)(x, key\u001b[39m=\u001b[39mkeys)\n\u001b[1;32m     60\u001b[0m loss \u001b[39m=\u001b[39m mse(pred_y, y)\n\u001b[0;32m---> 61\u001b[0m \u001b[39mprint\u001b[39m(jnp\u001b[39m.\u001b[39;49mmean(jnp\u001b[39m.\u001b[39;49mabs(pred_y))\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:728\u001b[0m, in \u001b[0;36m_forward_method_to_aval.<locals>.meth\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeth\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 728\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maval, name)\u001b[39m.\u001b[39;49mfun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:79\u001b[0m, in \u001b[0;36m_item\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcomplex\u001b[39m(a)\n\u001b[1;32m     78\u001b[0m \u001b[39melif\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(a\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[0;32m---> 79\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39;49m(a)\n\u001b[1;32m     80\u001b[0m \u001b[39melif\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(a\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger):\n\u001b[1;32m     81\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(a)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/jax/_src/core.py:1386\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> 1386\u001b[0m   \u001b[39mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "search_config = SearchConfig(key=jax.random.PRNGKey(0), num_configs=1)\n",
    "params = search_config[0]\n",
    "simulate(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from jaxtyping import Array\n",
    "from jax.random import KeyArray\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = SimpleNet(\n",
    "    in_features=2,\n",
    "    hidden_features=4,\n",
    "    out_features=1,\n",
    "    act=jnp.tanh,\n",
    "    # drop=0.,\n",
    "    key=key,\n",
    "    init_scale=1.\n",
    ")\n",
    "# model = MLP(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PjitFunction of <function jax.numpy.tanh at 0x11c28e2a0>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.28076008, dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "x, y = jnp.ones((batch_size, 2)), jnp.ones(batch_size)\n",
    "(key,) = jax.random.split(key, 1)\n",
    "model(x[0], key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred_y: Array, y: Array) -> Array:\n",
    "  \"\"\"Compute elementwise mean squared error.\"\"\"\n",
    "  return jnp.square(pred_y - y).mean(axis=-1)\n",
    "\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model: eqx.Module, x: Array, y: Array, key: KeyArray) -> Array:\n",
    "  \"\"\"Compute cross-entropy loss on a single example.\"\"\"\n",
    "  # print(x.shape, y.shape)\n",
    "  keys = jax.random.split(key, x.shape[0])\n",
    "  pred_y = jax.vmap(model)(x, key=keys)\n",
    "  loss = mse(pred_y, y)\n",
    "  # print(jnp.mean(jnp.abs(pred_y)).item())\n",
    "  return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PjitFunction of <function jax.numpy.tanh at 0x11c28e2a0>>\n"
     ]
    }
   ],
   "source": [
    "loss, grads = compute_loss(model, x, y, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grads(x[\u001b[39m0\u001b[39;49m], key\u001b[39m=\u001b[39;49mkey)\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/models/feedforward.py:205\u001b[0m, in \u001b[0;36mSimpleNet.__call__\u001b[0;34m(self, x, key)\u001b[0m\n\u001b[1;32m    203\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m    204\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact)\n\u001b[0;32m--> 205\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(x)\n\u001b[1;32m    206\u001b[0m x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m#.reshape(-1)\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "grads(x[0], key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.3679621 ,  0.56394064], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(model.__call__)(x[0], key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.07065082, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(model.act)(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-490.51846 0.0\n",
      "0.0 1.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([ 0.13280278,  0.67017764,  0.91039586,  0.82608676,  0.5739829 ,\n",
       "        0.4835764 ,  0.74268484,  1.1424717 ,  1.4003053 ,  1.5015821 ,\n",
       "        1.5188626 ,  1.4570681 ,  1.2281711 ,  0.697571  ,  0.00707251,\n",
       "       -0.41849583, -0.45780444, -0.20584969,  0.19321951,  0.5506563 ,\n",
       "        0.71367085,  0.6083238 ,  0.21559878, -0.3209988 , -0.73741674,\n",
       "       -0.910966  , -0.88841873, -0.72893816, -0.49133992, -0.27254838,\n",
       "       -0.19947001, -0.3623984 , -0.6866402 , -0.93569905, -0.9519884 ,\n",
       "       -0.6739141 , -0.12738809,  0.45165548,  0.8314931 ,  1.0004594 ,\n",
       "        1.0933323 ,  1.2417078 ,  1.4571066 ,  1.6044921 ,  1.6422333 ,\n",
       "        1.5736152 ,  1.2690396 ,  0.7049215 ,  0.39836252,  0.61761546,\n",
       "        1.0420426 ,  1.2518497 ,  1.139631  ,  0.5592598 , -0.4611649 ,\n",
       "       -1.2714162 , -1.5962808 , -1.681137  , -1.6906425 , -1.6432804 ,\n",
       "       -1.4295843 , -0.9486383 , -0.52866083, -0.46047392, -0.56591   ,\n",
       "       -0.5086306 , -0.04066715,  0.79095155,  1.4145088 ,  1.6177729 ,\n",
       "        1.6361613 ,  1.5332445 ,  1.1942732 ,  0.536683  , -0.33157966,\n",
       "       -1.1379014 , -1.5742406 , -1.6966176 , -1.7180603 , -1.7155845 ,\n",
       "       -1.6847075 , -1.5583531 , -1.1996483 , -0.53861433,  0.23259318,\n",
       "        0.7921762 ,  0.99216753,  0.78789043, -0.0480313 , -1.256607  ,\n",
       "       -1.7023237 , -1.7314675 , -1.7320299 , -1.7320424 , -1.7320052 ,\n",
       "       -1.7312503 , -1.7209573 , -1.6602507 , -1.4832462 , -1.137292  ],      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.scipy.special import erf as gain_function\n",
    "\n",
    "def Z(g):\n",
    "    return jnp.sqrt( (2/jnp.pi) * jnp.arcsin( (g**2) / (1 + (g**2)) ) )\n",
    "\n",
    "def generate_non_gaussian(key, xi, L, g):\n",
    "    C = jnp.abs(jnp.tile(jnp.arange(L)[:, jnp.newaxis], (1, L)) - jnp.tile(jnp.arange(L), (L, 1)))\n",
    "    C = -C ** 2 / (xi ** 2)\n",
    "    print(C.min(), C.max())\n",
    "    C = jnp.exp(C)#-C ** 2 / (xi ** 2))\n",
    "    print(C.min(), C.max())\n",
    "    print(jnp.linalg.norm(C - C.T))\n",
    "    z = jax.random.multivariate_normal(key, jnp.zeros(L, dtype=jnp.float32), C, method=\"svd\")\n",
    "    # print(z)\n",
    "    x = gain_function(g * z) / Z(g)\n",
    "    return x\n",
    "\n",
    "key = jax.random.PRNGKey(10)\n",
    "generate_non_gaussian(key, 4.47, 100, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-490.51844511508494 0.0\n",
      "9.344283683987794e-214 1.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.54891939, -1.51287565, -1.48423348, -1.4549705 , -1.37988648,\n",
       "       -1.17491775, -0.75829416, -0.22930115,  0.10143989,  0.01759758,\n",
       "       -0.49526407, -1.1405603 , -1.51700782, -1.63815109, -1.65584137,\n",
       "       -1.61534692, -1.48108181, -1.15562726, -0.52919494,  0.27638698,\n",
       "        0.81959246,  0.85928576,  0.31014739, -0.6276994 , -1.17190218,\n",
       "       -1.1413023 , -0.39232522,  0.98435388,  1.63800183,  1.72327758,\n",
       "        1.72940663,  1.72540715,  1.66188252,  1.13662467, -0.06246403,\n",
       "       -0.8875065 , -1.14644109, -1.11390957, -0.92821984, -0.72658798,\n",
       "       -0.64141659, -0.64801138, -0.56271989, -0.16161808,  0.61157206,\n",
       "        1.30576444,  1.58365093,  1.63551361,  1.58172505,  1.37797921,\n",
       "        0.96661578,  0.37972625, -0.37193023, -1.13257522, -1.55322124,\n",
       "       -1.67026464, -1.67974384, -1.61017492, -1.2964218 , -0.47742614,\n",
       "        0.51555853,  1.0602738 ,  1.13652464,  0.79209087, -0.03192705,\n",
       "       -0.90073183, -1.31342751, -1.36873313, -1.19169719, -0.85808697,\n",
       "       -0.57872429, -0.47933184, -0.48400669, -0.48034036, -0.39443693,\n",
       "       -0.1588366 ,  0.2623866 ,  0.77365008,  1.20340356,  1.48993259,\n",
       "        1.65067981,  1.71281813,  1.72735754,  1.72958776,  1.72790836,\n",
       "        1.7157741 ,  1.65034393,  1.38138747,  0.65327323, -0.43795705,\n",
       "       -1.2184822 , -1.49552759, -1.51697146, -1.36872548, -1.0611559 ,\n",
       "       -0.76816497, -0.64447799, -0.61315017, -0.51976582, -0.25463563])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf as gain_function\n",
    "\n",
    "def Z(g):\n",
    "    return np.sqrt( (2/np.pi) * np.arcsin( (g**2) / (1 + (g**2)) ) )\n",
    "\n",
    "def generate_non_gaussian(xi, L, g):\n",
    "    C = np.abs(np.tile(jnp.arange(L)[:, np.newaxis], (1, L)) - np.tile(np.arange(L), (L, 1)))\n",
    "    C = -C ** 2 / (xi ** 2)\n",
    "    print(C.min(), C.max())\n",
    "    C = np.exp(C)#-C ** 2 / (xi ** 2))\n",
    "    print(C.min(), C.max())\n",
    "    print(np.linalg.norm(C - C.T))\n",
    "    z = np.random.multivariate_normal(np.zeros(L, dtype=np.float32), C)\n",
    "    # print(z)\n",
    "    x = gain_function(g * z) / Z(g)\n",
    "    return x\n",
    "\n",
    "generate_non_gaussian(4.47, 100, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
