{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# from nets import datasets\n",
    "from datasets.nonlinear_gp import NonlinearGPDataset\n",
    "from nets.launch import configs\n",
    "from nets.models.feedforward import MLP\n",
    "from models.feedforward import SimpleNet\n",
    "from nets import samplers\n",
    "# from nets.simulators.online_sgd import train_step, eval_step, evaluate, simulate\n",
    "from experiments.online_sgd import simulate\n",
    "# from nets.experiments.analyzable_online_sgd.launcher_local import SearchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import field\n",
    "from nets.launch.hparams import Param\n",
    "from nets.launch.hparams import EnumParam\n",
    "from nets.launch.hparams import FixedParam\n",
    "\n",
    "@dataclass(frozen=True, kw_only=True)\n",
    "class SearchConfig(configs.Config):\n",
    "  \"\"\"Generic config for a hyperparameter search.\"\"\"\n",
    "\n",
    "  seed: Param = field(default_factory=lambda: FixedParam(0))\n",
    "\n",
    "  # Model params.\n",
    "  # num_ins: Param = field(default_factory=lambda: FixedParam(4))\n",
    "  num_hiddens: Param = field(default_factory=lambda: FixedParam(100))\n",
    "  init_scale: Param = field(default_factory=lambda: FixedParam(0.01))\n",
    "  num_dimensions: Param = field(default_factory=lambda: FixedParam(40))\n",
    "\n",
    "  # Training and evaluation params.\n",
    "  optimizer_fn: Param = field(default_factory=lambda: FixedParam(optax.adam))\n",
    "  learning_rate: Param = field(default_factory=lambda: FixedParam(1e-3))\n",
    "  batch_size: Param = field(default_factory=lambda: FixedParam(100))\n",
    "  num_epochs: Param = field(default_factory=lambda: FixedParam(1))\n",
    "\n",
    "  # Dataset params.\n",
    "  dataset_cls: Param = field(default_factory=lambda: FixedParam(NonlinearGPDataset))\n",
    "  xi1: Param = field(default_factory=lambda: FixedParam(0.1))\n",
    "  xi2: Param = field(default_factory=lambda: FixedParam(1.1))\n",
    "  gain: Param = field(default_factory=lambda: FixedParam(1.0))\n",
    "  # num_dimensions: Param = field(default_factory=lambda: FixedParam(4))\n",
    "  sampler_cls: Param = field(default_factory=lambda: FixedParam(samplers.EpochSampler))\n",
    "  \n",
    "#   dataset_cls: Param = field(init=False)\n",
    "#   num_dimensions: Param = field(init=False)\n",
    "#   num_exemplars_per_class: Param = field(init=False)\n",
    "#   exemplar_noise_scale: Param = field(init=False)\n",
    "\n",
    "  # Sampler params.\n",
    "#   sampler_cls: Param = field(init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend: cpu\n",
      "\n",
      "Using configuration:\n",
      "{'batch_size': 100,\n",
      " 'dataset_cls': <class 'datasets.nonlinear_gp.NonlinearGPDataset'>,\n",
      " 'gain': 1.0,\n",
      " 'init_scale': 0.01,\n",
      " 'learning_rate': 0.001,\n",
      " 'num_dimensions': 40,\n",
      " 'num_epochs': 1,\n",
      " 'num_hiddens': 100,\n",
      " 'optimizer_fn': <function adam at 0x13b186fc0>,\n",
      " 'sampler_cls': <class 'nets.samplers.base.EpochSampler'>,\n",
      " 'seed': 0,\n",
      " 'xi1': 0.1,\n",
      " 'xi2': 1.1}\n",
      "\n",
      "simulate: len(dataset)=1000\n",
      "Model:\n",
      "SimpleNet(\n",
      "  fc1=Linear(\n",
      "    weight=f32[100,40],\n",
      "    bias=f32[100],\n",
      "    in_features=40,\n",
      "    out_features=100,\n",
      "    use_bias=True\n",
      "  ),\n",
      "  act=<wrapped function <lambda>>\n",
      ")\n",
      "\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation over 1000 examples in 1.35 secs.\n",
      "####\n",
      "ITERATION 0\n",
      "eval set:\n",
      "\n",
      "\tloss:\t\t\t1.0002896785736084\n",
      "\taccuracy:\t\t0.0\n",
      "\tBASELINE:\t\t50.00%\n",
      "\tGT labels:\t\t[ 1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1.\n",
      " -1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1. -1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.\n",
      "  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.\n",
      "  1.  1.  1. -1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.\n",
      " -1.  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      " -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1.  1. -1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1. -1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      " -1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.\n",
      " -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.\n",
      "  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1.\n",
      "  1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
      " -1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1.\n",
      " -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1.  1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.\n",
      "  1. -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.\n",
      " -1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.]\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape float32[].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m search_config \u001b[39m=\u001b[39m SearchConfig(key\u001b[39m=\u001b[39mjax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(\u001b[39m0\u001b[39m), num_configs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m params \u001b[39m=\u001b[39m search_config[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m simulate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/experiments/online_sgd.py:351\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(seed, num_hiddens, init_scale, optimizer_fn, learning_rate, batch_size, num_epochs, dataset_cls, xi1, xi2, gain, num_dimensions, sampler_cls)\u001b[0m\n\u001b[1;32m    349\u001b[0m (train_key,) \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(train_key, \u001b[39m1\u001b[39m)\n\u001b[1;32m    350\u001b[0m train_step_num \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mnext\u001b[39m(itercount))\n\u001b[0;32m--> 351\u001b[0m train_loss, model, opt_state \u001b[39m=\u001b[39m train_step(\n\u001b[1;32m    352\u001b[0m   model, optimizer, opt_state, x, y, train_key\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m train_step_num \u001b[39m%\u001b[39m evaluation_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(sampler):\n\u001b[1;32m    356\u001b[0m   metrics\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    357\u001b[0m     evaluate(\n\u001b[1;32m    358\u001b[0m       iteration\u001b[39m=\u001b[39mtrain_step_num,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     )\n\u001b[1;32m    365\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/equinox/_jit.py:107\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39mFalse\u001b[39;49;00m, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/equinox/_jit.py:103\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached(dynamic, static)\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached(dynamic, static)\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m _postprocess(out)\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/experiments/online_sgd.py:75\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, opt_state, x, y, key)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@eqx\u001b[39m\u001b[39m.\u001b[39mfilter_jit\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\n\u001b[1;32m     67\u001b[0m   model: eqx\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m   key: KeyArray,\n\u001b[1;32m     73\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Array, eqx\u001b[39m.\u001b[39mModule, Array]:\n\u001b[1;32m     74\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Train the model on a single example.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m   loss, grads \u001b[39m=\u001b[39m compute_loss(model, x, y, key)\n\u001b[1;32m     76\u001b[0m   \u001b[39m# print(\"grads l2 norm: \", jnp.linalg.norm(grads(x, key=key)))\u001b[39;00m\n\u001b[1;32m     77\u001b[0m   \u001b[39m# print(grads)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m   updates, opt_state \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mupdate(grads, opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/experiments/online_sgd.py:61\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(model, x, y, key)\u001b[0m\n\u001b[1;32m     59\u001b[0m pred_y \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(model)(x, key\u001b[39m=\u001b[39mkeys)\n\u001b[1;32m     60\u001b[0m loss \u001b[39m=\u001b[39m mse(pred_y, y)\n\u001b[0;32m---> 61\u001b[0m \u001b[39mprint\u001b[39m(jnp\u001b[39m.\u001b[39;49mmean(jnp\u001b[39m.\u001b[39;49mabs(pred_y))\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:728\u001b[0m, in \u001b[0;36m_forward_method_to_aval.<locals>.meth\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeth\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 728\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maval, name)\u001b[39m.\u001b[39;49mfun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:79\u001b[0m, in \u001b[0;36m_item\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcomplex\u001b[39m(a)\n\u001b[1;32m     78\u001b[0m \u001b[39melif\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(a\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[0;32m---> 79\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39;49m(a)\n\u001b[1;32m     80\u001b[0m \u001b[39melif\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(a\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger):\n\u001b[1;32m     81\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(a)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.11/site-packages/jax/_src/core.py:1386\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> 1386\u001b[0m   \u001b[39mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "search_config = SearchConfig(key=jax.random.PRNGKey(0), num_configs=1)\n",
    "params = search_config[0]\n",
    "simulate(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from jaxtyping import Array\n",
    "from jax.random import KeyArray\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = SimpleNet(\n",
    "    in_features=2,\n",
    "    hidden_features=4,\n",
    "    out_features=1,\n",
    "    act=jnp.tanh,\n",
    "    # drop=0.,\n",
    "    key=key,\n",
    "    init_scale=1.\n",
    ")\n",
    "# model = MLP(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PjitFunction of <function jax.numpy.tanh at 0x11c28e2a0>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.28076008, dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "x, y = jnp.ones((batch_size, 2)), jnp.ones(batch_size)\n",
    "(key,) = jax.random.split(key, 1)\n",
    "model(x[0], key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred_y: Array, y: Array) -> Array:\n",
    "  \"\"\"Compute elementwise mean squared error.\"\"\"\n",
    "  return jnp.square(pred_y - y).mean(axis=-1)\n",
    "\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model: eqx.Module, x: Array, y: Array, key: KeyArray) -> Array:\n",
    "  \"\"\"Compute cross-entropy loss on a single example.\"\"\"\n",
    "  # print(x.shape, y.shape)\n",
    "  keys = jax.random.split(key, x.shape[0])\n",
    "  pred_y = jax.vmap(model)(x, key=keys)\n",
    "  loss = mse(pred_y, y)\n",
    "  # print(jnp.mean(jnp.abs(pred_y)).item())\n",
    "  return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PjitFunction of <function jax.numpy.tanh at 0x11c28e2a0>>\n"
     ]
    }
   ],
   "source": [
    "loss, grads = compute_loss(model, x, y, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leonlufkin/Documents/GitHub/Localization/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grads(x[\u001b[39m0\u001b[39;49m], key\u001b[39m=\u001b[39;49mkey)\n",
      "File \u001b[0;32m~/Documents/GitHub/Localization/models/feedforward.py:205\u001b[0m, in \u001b[0;36mSimpleNet.__call__\u001b[0;34m(self, x, key)\u001b[0m\n\u001b[1;32m    203\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m    204\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact)\n\u001b[0;32m--> 205\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(x)\n\u001b[1;32m    206\u001b[0m x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m#.reshape(-1)\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "grads(x[0], key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.3679621 ,  0.56394064], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(model.__call__)(x[0], key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.07065082, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(model.act)(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
